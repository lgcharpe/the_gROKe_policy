{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "06a114c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fcf1da",
   "metadata": {},
   "source": [
    "# Step 1: Defining the Environment using Gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a361565",
   "metadata": {},
   "source": [
    "User guide for defining custom environments: https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/\n",
    "\n",
    "The only thing we need to do, really, is to map the Microgrid to an observation space, an action space and a reward function. We also need to implement a reset and step function that returns the current state.\n",
    "\n",
    "For our actions, we have [produce, not produce] for Wind, Solar and Gas. Let's just start by having it binary, so that the production is either at max capacity or zero at a given time step.\n",
    "\n",
    "The reward is just the Cost function from the assignment text at a time step.\n",
    "\n",
    "The thing I struggle to wrap my head around, is the observation space. Is it here that we include the data from the files? So that the \"state\" of our environment is how much energy is produced given an action sequence and the wind speed/ solar irradience at a specific time step?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567012a0",
   "metadata": {},
   "source": [
    "### Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9be520",
   "metadata": {},
   "source": [
    "- Our \"episode\" is one hour. But how many iterations per episode? Could we just get rid of this level? I see that most people use episodes*iterations_per_episode as the total number of steps, but we just do one step per hour becouse of our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e21591d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Microgrid(gym.Env):\n",
    "\n",
    "    def __init__(self, microcrid_settings = {}):\n",
    "        \n",
    "        \n",
    "        #TODO: Define observation space\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"solarIrradience\": ...,\n",
    "                \"wind\": ...,\n",
    "                \"load\": ... \n",
    "            }\n",
    "        )\n",
    "\n",
    "        #TODO: Define action space\n",
    "        self.action_space = spaces.Discrete(3*2) #Produce/Not-produce for 3 energy producing nodes\n",
    "        \n",
    "   \n",
    "    #TODO: Define the _get_obs method\n",
    "    def _get_obs(self):\n",
    "        '''\n",
    "        Private method that translates the environmentâ€™s state into an observation.\n",
    "        Could be useful to do it here instead of inside the reset.\n",
    "        '''\n",
    "        pass\n",
    "    \n",
    "    #TODO: Define the Reset method\n",
    "    def reset(self, seed=None, options=None):\n",
    "    # We need the following line to seed self.np_random\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        return observation\n",
    "    \n",
    "    #TODO: Define the Step method\n",
    "    def step(self, action):\n",
    "        \n",
    "        #Return the state of the environment. I guess this is the energy produced at this time step?\n",
    "        observation = self._get_obs()\n",
    "        \n",
    "        #We only terminate after episodes, so I guess this is uneccesary?\n",
    "        terminated = ...\n",
    "        \n",
    "        #I guess the reward is the OperationCost from the Microgrid class?\n",
    "        reward = ...\n",
    "\n",
    "\n",
    "        return observation, reward, terminated, False, info\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011f9f4b",
   "metadata": {},
   "source": [
    "# Step 2: Define Q learning procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa21c3",
   "metadata": {},
   "source": [
    "### Example using a pre-defined environment from Gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bcba7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Env\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"rgb_array\")\n",
    "n_observations = env.observation_space.n\n",
    "n_actions = env.action_space.n\n",
    "Q_table = np.zeros((n_observations,n_actions))\n",
    "\n",
    "# Cosntants\n",
    "n_episodes = 10000\n",
    "max_iter_episode = 1000\n",
    "exploration_proba = 1\n",
    "exploration_decreasing_decay = 0.001\n",
    "min_exploration_proba = 0.01\n",
    "gamma = 0.99\n",
    "lr = 0.1\n",
    "rewards_per_episode = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aa6dfdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward per thousand episodes\n",
      "1000. Mean espiode reward: 0.047\n",
      "2000. Mean espiode reward: 0.216\n",
      "3000. Mean espiode reward: 0.414\n",
      "4000. Mean espiode reward: 0.624\n",
      "5000. Mean espiode reward: 0.722\n",
      "6000. Mean espiode reward: 0.709\n",
      "7000. Mean espiode reward: 0.703\n",
      "8000. Mean espiode reward: 0.715\n",
      "9000. Mean espiode reward: 0.73\n",
      "10000. Mean espiode reward: 0.709\n"
     ]
    }
   ],
   "source": [
    "for e in range(n_episodes):\n",
    "    current_state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    #sum the rewards that the agent gets from the environment\n",
    "    total_episode_reward = 0\n",
    "    \n",
    "    if type(current_state) != int:\n",
    "            current_state = current_state[0]\n",
    "    \n",
    "    for i in range(max_iter_episode): \n",
    "        if np.random.uniform(0,1) < exploration_proba:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state,:])\n",
    "        \n",
    "        # The environment runs the chosen action and returns\n",
    "        # the next state, a reward and true if the epiosed is ended.\n",
    "        next_state, reward, done, _, _ = env.step(action)        \n",
    "        \n",
    "        # We update our Q-table using Bellman equation, basically. \n",
    "        Q_table[current_state, action] = (1-lr) * Q_table[current_state, action] +lr*(reward + gamma*max(Q_table[next_state,:]))\n",
    "        total_episode_reward = total_episode_reward + reward\n",
    "        \n",
    "        # If the episode is finished, we leave the for loop\n",
    "        if done:\n",
    "            break\n",
    "        current_state = next_state\n",
    "        \n",
    "    #We update the exploration proba using exponential decay formula \n",
    "    exploration_proba = max(min_exploration_proba, np.exp(-exploration_decreasing_decay*e))\n",
    "    rewards_per_episode.append(total_episode_reward)\n",
    "\n",
    "print(\"Mean reward per thousand episodes\")\n",
    "for i in range(10):\n",
    "    print(f\"{(i+1)*1000}. Mean espiode reward: {np.mean(rewards_per_episode[1000*i:1000*(i+1)])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c41a502",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
